require(distill)
create_post('Install Docker Engine')
require(distill)
require(distill)
create_post('Shiny Proxy' )
create_post('Shiny app in a container' )
shiny::runApp('_posts/2020-09-19-shiny-app-in-a-container')
sudo docker run -it --rm -p 9393:9393 iris
runApp('_posts/2020-09-19-shiny-app-in-a-container/iris')
create_post('How to dockerize ShinyProxy')
create_post('How to move docker data direcory')
require(distill)
create_post('Install Portainer')
knitr::opts_chunk$set(echo = FALSE)
require(distill)
create_post('Install ShinyProxy')
install.packages('odbc')
con <- dbConnect(MySQL(),
user='dwhuser',
password='dwhuserpwd',
dbname='dwh',
host='10.31.68.21')
requiore(odbc)
require(odbc)
con <- dbConnect(MySQL(),
user='dwhuser',
password='dwhuserpwd',
dbname='dwh',
host='10.31.68.21')
install.packages('RMYSQL')
install.packages('RMySQL')
con <- dbConnect(MySQL(),
user='dwhuser',
password='dwhuserpwd',
dbname='dwh',
host='10.31.68.21')
require(RMySQL)
install.packages('RMySQL')
con <- dbConnect(MySQL(),
user='dwhuser',
password='dwhuserpwd',
dbname='dwh',
host='10.31.68.21')
require(RMySQL)
con <- dbConnect(MySQL(),
user='dwhuser',
password='dwhuserpwd',
dbname='dwh',
host='10.31.68.21')
rm(list = ls())
requie(distill)
require(distill)
create_post('minicran')
install.packages('minicrarn')
install.packages('miniCRAN')
require(miniCRAN)
?pkgDep
repo <- 'https://cran.wu.ac.at/'
my_pkgs <- 'tidyverse'
all_pkgs <- pkgDep(my_pkg, repo = repo, type = 'source', suggest = FALSE)
repo <- 'https://cran.wu.ac.at/'
my_pkgs <- 'tidyverse'
all_pkgs <- pkgDep(my_pkgs, repo = repo, type = 'source', suggest = FALSE)
all_pkgs
repo <- 'https://cran.wu.ac.at/'
my_pkgs <- 'tidyverse'
all_pkgs <- pkgDep(my_pkgs, repo = repo, type = 'source', suggest = FALSE, depends = TRUE)
all_pkgs
dir.create(minicran_path, 'minicran')
minicran_path <- '/data/minicran'
dir.create(minicran_path, 'minicran')
minicran_path <- '/data/minicran'
dir.create(minicran_path, 'minicran')
path <- '/data/minicran'
dir.create(path, 'minicran')
makeRepo(all_pkgs, path = path , repo = repo, type = 'source')
knitr::opts_chunk$set(echo = T)
require(miniCRAN)
repo <- 'https://cran.wu.ac.at/'
my_pkgs <- 'tidyverse'
all_pkgs <- pkgDep(my_pkgs, repo = repo, type = 'source', suggest = FALSE)
all_pkgs
install.packages(pkgs = 'utf8'. repo = 'file:///data/minicran/')
install.packages(pkgs = 'utf8', repo = 'file:///data/minicran/')
addPackage('scuba', , path = path , repo = repo, type = 'source')
addPackage('scuba', , path = path , repo = repo, type = 'source')
addLocalPackage()
addLocalPackage
?addLocalPackage
search()
ls(2)
?addPackageListingGithub
addGithubPackage(githubPath="https://github.com/jalvesaq/colorout",path=path,build=TRUE)
addGithubPackage <- function(githubPath,...){
packageName <- basename(githubPath)
exDir <- file.path(tempdir(),packageName)
if(file.exists(exDir)) unlink(exDir, recursive=TRUE)
zipFile <- file.path(tempdir(),paste0(packageName,".zip"))
download.file(paste0(githubPath,"/","zipball/master"),zipFile)
unzip(zipFile, exdir = exDir)
file.rename(list.files(exDir,full.names=TRUE)[1],file.path(exDir,packageName))
addLocalPackage(packageName,exDir,...)
}
addGithubPackage(githubPath="https://github.com/jalvesaq/colorout",path=path,build=TRUE)
install.packages(pkgs = 'colorout', repo = 'file:///data/minicran/')
updatePackages(path = path, repo = repo, type = "source", ask = FALSE)
ls()
rm(list = ls())
ls()
available.packages()
head(available.packages())
i <- head(available.packages())
class(i)
dimnames(i)
require(distill)
create_post('Data Analysis with Spraklyr')
knitr::opts_chunk$set(echo = TRUE)
require(sparklyr)
require(dplyr)
require(readr)
require(ggplot2)
# global spark memory
Sys.setenv("SPARK_MEM" = "12g")
# Initialize configuration with defaults
config <- spark_config()
# Memory
config["sparklyr.shell.driver-memory"] <- "12g"
# Cores
config["sparklyr.connect.cores.local"] <- 4
sc <- spark_connect(master = "local", version = "2.3", config = config)
sc <- spark_connect(master = "local", version = "2.1", config = config)
spark_install(version = "3.0.1")
spark_available_versions()
spark_install(version = "3.0.0")
knitr::opts_chunk$set(echo = TRUE)
require(sparklyr)
require(dplyr)
require(readr)
require(ggplot2)
# global spark memory
Sys.setenv("SPARK_MEM" = "12g")
# Initialize configuration with defaults
config <- spark_config()
# Memory
config["sparklyr.shell.driver-memory"] <- "12g"
# Cores
config["sparklyr.connect.cores.local"] <- 4
sc <- spark_connect(master = "local", version = "3.0", config = config)
system('du -h ~/tmp/data')
system('du -h /data/airline')
system('du -h /data/.airline/csv')
system.time({
d_all <- spark_read_csv(sc ,
name = 'd_all',
path = "file:///data/.airline/csv",
header = T)
})
system('rm -rf /data/airline/prq/')
system.time({
spark_write_parquet(d_all, path = '/data/airline/prq/')
})
system('rm -rf /data/.airline/prq/')
system.time({
spark_write_parquet(d_all, path = '/data/.airline/prq/')
})
system.time({
spark_write_parquet(d_all, path = '/data/.airline/prq/')
})
system('rm -rf /data/.airline/prq/')
system.time({
spark_write_parquet(d_all, path = '/data/.airline/prq/')
})
d_all
system.time({
spark_write_parquet(d_all, path = '/data/.airline/prq/')
})
system('du -h /data/.airline/prq/')
system('du -h /data/.airline/prq/')
system.time({
d_all <- spark_read_parquet(sc ,
name = 'd_all',
path = "file:///home/andrea/tmp/parquet/",
header = T)
})
system.time({
d_all <- spark_read_parquet(sc ,
name = 'd_all',
path = "file:///data/.airline/prq/",
header = T)
})
system.time({
n <- d_all %>%
summarise(n = n()) %>%
collect()
print(n)
})
system.time({
d_model <- d_all %>%
mutate(ArrDelay = as.numeric(ArrDelay) ,
DepDelay = as.numeric(DepDelay) ,
Distance = as.numeric(Distance)) %>%
filter(!is.na(ArrDelay) & !is.na(DepDelay) & !is.na(Distance)) %>%
filter(DepDelay > -10 & DepDelay < 240) %>%
filter(ArrDelay > -60 & ArrDelay < 360) %>%
mutate(Gain = DepDelay - ArrDelay) %>%
select(Year, Month, ArrDelay, DepDelay, Distance, UniqueCarrier, Gain)
})
system.time({
d_model %>%
summarise(n = n()) %>%
collect()
})
d_carrier <- spark_read_csv(sc ,
name = 'd_carrier',
path = "file:///data/.arline/csv/carriers.csv",
header = T)
d_carrier <- spark_read_csv(sc ,
name = 'd_carrier',
path = "file:///data/.airline/csv/carriers.csv",
header = T)
system.time({
d_model <- d_model %>%
left_join(d_carrier, by = c("UniqueCarrier" = "Code"))
})
system.time({
d_model_2008 <- d_model %>% filter(Year == 2008)
d_model <- d_model %>% filter(Year <= 2007)
})
system.time({
n <- d_model %>%  summarise(n = n())
n2008 <- d_model_2008 %>%  summarise(n = n())
print(n)
print(n2008)
})
system.time({
model_partition <- d_model %>%
sdf_random_split(d_trn = 0.8, d_tst = 0.2, seed = 5555)
})
system.time({
fm <-  ml_linear_regression(model_partition$d_trn,
formula = Gain ~ Distance + DepDelay + UniqueCarrier)
})
system.time({
summary(fm)
})
system.time({
model_deciles <- lapply(model_partition,
function(x) {
ml_predict(fm, x) %>%
mutate(Decile = ntile(desc(prediction), 10)) %>%
group_by(Decile) %>%
summarize(Gain = mean(Gain)) %>%
select(Decile, Gain) %>%
collect()}
)
})
d_decile <- bind_rows(
as_tibble(model_deciles$d_trn) %>% mutate(partition = 'trn'),
as_tibble(model_deciles$d_tst) %>% mutate(partition = 'tst'))
d_decile %>%
ggplot(aes(factor(Decile), Gain, fill = partition)) +
geom_bar(stat = 'identity', position = 'dodge') +
labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes')
system.time({
pred_2008 <- ml_predict(fm, d_model_2008) %>%
group_by(Description) %>%
summarize(Gain = mean(Gain), prediction = mean(prediction), freq = n()) %>%
filter(freq > 10000) %>%
collect()
})
ggplot(pred_2008, aes(Gain, prediction)) +
geom_point(alpha = 0.75, color = 'red', shape = 3) +
geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') +
geom_text(aes(label = substr(Description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) +
labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted')
pred_2008
spark_disconnect(sc)
require(sparklyr)
require(dplyr)
require(readr)
require(ggplot2)
Sys.setenv("SPARK_MEM" = "12g")
config <- spark_config()
config["sparklyr.shell.driver-memory"] <- "12g"
# Cores
config["sparklyr.connect.cores.local"] <- 4
sc <- spark_connect(master = "local", version = "3.0", config = config)
system('du -h /data/.airline/csv')
system.time({
d_all <- spark_read_csv(sc ,
name = 'd_all',
path = "file:///data/.airline/csv",
header = T)
})
system('rm -rf /data/.airline/prq/')
system.time({
spark_write_parquet(d_all, path = '/data/.airline/prq/')
})
sc <- spark_connect(master = "local", version = "3.0", config = config)
spark_disconnect(sc)
sc <- spark_connect(master = "local", version = "3.0", config = config)
system('du -h /data/.airline/csv')
system('du -h /data/.airline/csv', intern = T )
print (sys)
sys <- system('du -h /data/.airline/csv', intern = T )
print (sys)
sys <- system('du -h /data/.airline/csv', intern = F )
sys
cat(system('du -h /data/.airline/csv') , '\n')
ggplot(pred_2008, aes(Gain, prediction)) +
geom_point(alpha = 0.75, color = 'red', shape = 3) +
geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') +
geom_text(aes(label = substr(Description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) +
labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted')
path <- "/data/airline2"
db <- "ontime.sqlite"
path_db <- paste(path, db, sep = "/")
db_exists <- list.files(path, pattern = db)
db_exists
if(length(db_exists) != 0) system(paste("rm", path_db))
con <- dbConnect(RSQLite::SQLite(), path_db)
require(tidyverse)
require(DBI)
require(RSQLite)
require(RSQLite)
path <- "/data/airline2"
db <- "ontime.sqlite"
path_db <- paste(path, db, sep = "/")
db_exists <- list.files(path, pattern = db)
if(length(db_exists) != 0) system(paste("rm", path_db))
con <- dbConnect(RSQLite::SQLite(), path_db)
files <- list.files("path", pattern = ".csv", full.names = TRUE)
files
files <- list.files(path, pattern = ".csv", full.names = TRUE)
files
for (i in 1:length(files)){
head <- ifelse (i == 1, TRUE, FALSE)
skip <- ifelse (i == 1, 0, 1)
append <- ifelse (i == 1, FALSE, TRUE)
this_file <- files[i]
encoding <- charset(this_file)
df <- read.table(this_file, sep = ",", head = head, encoding = encoding, skip = skip, nrows = 100)
dbWriteTable(conn = con, name = "ontime", value = df ,append = append)
cat(date() , this_file, "loaded", "\n")
rm(df)
}
charset <- function (file){
info <- system(paste("file -i" ,file ), intern = TRUE)
info_split <- strsplit(info, split = "charset=")[[1]][2]
info_split
}
for (i in 1:length(files)){
head <- ifelse (i == 1, TRUE, FALSE)
skip <- ifelse (i == 1, 0, 1)
append <- ifelse (i == 1, FALSE, TRUE)
this_file <- files[i]
encoding <- charset(this_file)
df <- read.table(this_file, sep = ",", head = head, encoding = encoding, skip = skip, nrows = 100)
dbWriteTable(conn = con, name = "ontime", value = df ,append = append)
cat(date() , this_file, "loaded", "\n")
rm(df)
}
and by using a for loop you can load all the files into the newly created data base:
i <- 1
```{r eval=FALSE}
for (i in 1:length(files)){
head <- ifelse (i == 1, TRUE, FALSE)
skip <- ifelse (i == 1, 0, 1)
append <- ifelse (i == 1, FALSE, TRUE)
this_file <- files[i]
encoding <- charset(this_file)
df <- read.table(this_file, sep = ",", head = head, encoding = encoding, skip = skip, nrows = 100)
dbWriteTable(conn = con, name = "ontime", value = df ,append = append)
cat(date() , this_file, "loaded", "\n")
rm(df)
}
```
i <- 1
head <- ifelse (i == 1, TRUE, FALSE)
skip <- ifelse (i == 1, 0, 1)
append <- ifelse (i == 1, FALSE, TRUE)
this_file <- files[i]
this_file
encoding <- charset(this_file)
encoding
df <- readr::read_delim(this_file, delim = ",", n_max = 100) # , head = head, encoding = encoding, skip = skip, nrows = 100)
df <- readr::read_delim(this_file, delim = ",", n_max = 100, col_types = cols()) # , head = head, encoding = encoding, skip = skip, nrows = 100)
df
dbWriteTable(conn = con, name = "ontime", value = df ,append = append)
append <- ifelse (i == 1, FALSE, TRUE)
dbWriteTable(conn = con, name = "ontime", value = df ,append = append)
append
-append
append <- ifelse (i == 1, FALSE, TRUE)
append
and by using a for loop you can load all the files into the newly created data base:
i <- 1
```{r eval=FALSE}
for (i in 1:length(files)){
# head <- ifelse (i == 1, TRUE, FALSE)
# skip <- ifelse (i == 1, 0, 1)
append <- ifelse (i == 1, FALSE, TRUE)
overwrite <- ifelse (i == 1, TRUE, FALSE)
ppend <- ifelse (i == 1, FALSE, TRUE)
append
append
dbWriteTable(conn = con, name = "ontime", value = df ,append = append, overwrite = overwrite)
overwrite <- ifelse (i == 1, TRUE, FALSE)
dbWriteTable(conn = con, name = "ontime", value = df ,append = append, overwrite = overwrite)
cat(date() , this_file, "loaded", "\n")
rm(i)
for (i in 1:length(files)){
# head <- ifelse (i == 1, TRUE, FALSE)
# skip <- ifelse (i == 1, 0, 1)
append <- ifelse (i == 1, FALSE, TRUE)
overwrite <- ifelse (i == 1, TRUE, FALSE)
this_file <- files[i]
encoding <- charset(this_file)
df <- readr::read_delim(this_file, delim = ",", n_max = 100, col_types = cols()) # , head = head, encoding = encoding, skip = skip, nrows = 100)
dbWriteTable(conn = con, name = "ontime", value = df ,append = append, overwrite = overwrite)
cat(date() , this_file, "loaded", "\n")
rm(df)
}
dbDisconnect(con)
con_dplyr <- src_sqlite(path_db)
ontime <- tbl(con_dplyr, "ontime")
class(ontime)
dim(ontime)
ontime_stat <- ontime %>% group_by(Year, Month) %>%
summarise(avg = mean(DepDelay), min = min(DepDelay), max = max(DepDelay))
ontime_stat
class(ontime_stat)
ontime_dep_delay <- ontime %>%
select(year = Year, dep_delay = DepDelay, arr_delay = ArrDelay) %>%
filter(dep_delay > 0) %>%
collect()
pl <- ggplot(ontime_dep_delay , aes(dep_delay, arr_delay))
pl <- pl + stat_binhex(bins = 30) + facet_wrap(~year, ncol = 2)
print(pl)
create_post('R and SQLlite ')
require(distill)
create_post('R and SQLlite ')
install.packages('formatR')
devtools::install_github("datastorm-open/DependenciesGraphs")
require(skyad)
require(DependenciesGraph)
require("DependenciesGraphs")
dep <- funDependencies('package:skyad', 'diagmostiic')
dep <- funDependencies('package:skyad', 'diagmostic')
dep <- funDependencies('package:skyad', 'diagnosis')
plot(dep)
require(rstudioapi)
?jobRunScript
278/2
save.image
readr::save_rds
readr::save_RDS
saveRDS
?save
readr::read_rds
require(rstudioapi)
jobRunScript
runScriptJob
rstudioapi:::runScriptJob
?callFun
callFun("runScriptJob")
rstudioapi:::runScriptJob
